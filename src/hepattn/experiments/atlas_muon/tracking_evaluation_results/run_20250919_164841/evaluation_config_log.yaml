baseline_filtering_stats:
  baseline_tracks: 0
  rejected_tracks: 19
  total_tracks: 19
data_directory: /scratch/ml_test_data_156000_hdf5_filtered_wp0990_maxtrk2_maxhit500
evaluation_file: /eos/project/e/end-to-end-muon-tracking/tracking/data/best2tracktracking/TRK-ATLAS-Muon-smallModel_20250915-T192111/ckpts/epoch=008-val_loss=1.62751_ml_test_data_156000_hdf5_filtered_wp0990_maxtrk2_maxhit500_eval.h5
evaluation_timestamp: '2025-09-19 16:48:54'
max_events: 10
results_by_region:
  all_tracks:
    task1_hit_track_assignment:
      average_efficiency: 0.29088742599440015
      average_fake_rate: 0.03700742688604428
      efficiency_std: 0.44514711867685025
      fake_rate_std: 0.06711285476097263
      roc_auc: 0.9318186308051775
      total_hits: 1594
      true_assignments: 517
    task2_track_validity:
      average_efficiency: 0.35
      average_fake_rate: 0.0
      efficiency_std: 0.4769696007084728
      fake_rate_std: 0.0
      roc_auc: .nan
      total_tracks: 19
      valid_tracks: 19
  baseline_filtered: {}
  rejected_tracks:
    task1_hit_track_assignment:
      average_efficiency: 0.2
      average_fake_rate: 0.026623994915927242
      efficiency_std: 0.4000000000000001
      fake_rate_std: 0.05518547173020552
      roc_auc: 0.9436645688860383
      total_hits: 752
      true_assignments: 247
    task2_track_validity:
      average_efficiency: 0.25
      average_fake_rate: 0.0
      efficiency_std: 0.4330127018922193
      fake_rate_std: 0.0
      roc_auc: .nan
      total_tracks: 9
      valid_tracks: 9
